{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jibon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transformers version: 4.38.2\n",
      "PyTorch version: 2.0.1+cu118\n",
      "CUDA available: True\n",
      "Using device: cuda\n",
      "Loaded dataset with 12900 entries\n",
      "                                             content           label\n",
      "0  The CDC currently reports 99031 deaths. In gen...        Reliable\n",
      "1  States reported 1121 deaths a small rise from ...        Reliable\n",
      "2  Politically Correct Woman (Almost) Uses Pandem...  Misinformation\n",
      "3  #IndiaFightsCorona: We have 1524 #COVID testin...        Reliable\n",
      "4  Populous states can generate large case counts...        Reliable\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "Reliable          6719\n",
      "Misinformation    6181\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing retrieval function on sample tweets:\n",
      "\n",
      "TWEET: the cdc currently reports 99031 deaths. in general the discrepancies in death counts between differe ...\n",
      "RETRIEVED: GENERAL INFO: COVID-19 is caused by the SARS-CoV-2 virus and spreads primarily through respiratory d ...\n",
      "\n",
      "TWEET: states reported 1121 deaths a small rise from last tuesday. southern states reported 640 of those de ...\n",
      "RETRIEVED: GENERAL INFO: COVID-19 is caused by the SARS-CoV-2 virus and spreads primarily through respiratory d ...\n",
      "\n",
      "TWEET: politically correct woman (almost) uses pandemic as excuse not to reuse plastic bag #coronavirus #na ...\n",
      "RETRIEVED: GENERAL INFO: COVID-19 is caused by the SARS-CoV-2 virus and spreads primarily through respiratory d ...\n",
      "\n",
      "TWEET: #indiafightscorona: we have 1524 #covid testing laboratories in india and as on 25th august 2020 368 ...\n",
      "RETRIEVED: FACT CHECK: COVID-19 tests are designed to detect current or past infection with the SARS-CoV-2 viru ...\n",
      "\n",
      "TWEET: populous states can generate large case counts but if you look at the new cases per million today 9  ...\n",
      "RETRIEVED: GENERAL INFO: COVID-19 is caused by the SARS-CoV-2 virus and spreads primarily through respiratory d ...\n",
      "\n",
      "Using a sample of 1000 tweets to avoid computational overhead\n",
      "\n",
      "Retrieving external knowledge for tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 10037.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples of retrieved knowledge:\n",
      "\n",
      "TWEET: bill gates who is supporting covid-19 vaccine research visited in new zealand during may. ...\n",
      "RETRIEVED: FACT CHECK: Claims that Bill Gates is using vaccines for population control are false. ...\n",
      "LABEL: Misinformation\n",
      "\n",
      "TWEET: pak pm imran khan's wife tested positive for covid-19. ...\n",
      "RETRIEVED: GENERAL INFO: COVID-19 is caused by the SARS-CoV-2 virus and spreads primarily through respiratory d ...\n",
      "LABEL: Misinformation\n",
      "\n",
      "TWEET: ???clearly, the obama administration did not leave any kind of game plan for something like this.??� ...\n",
      "RETRIEVED: GENERAL INFO: COVID-19 is caused by the SARS-CoV-2 virus and spreads primarily through respiratory d ...\n",
      "LABEL: Misinformation\n",
      "\n",
      "TWEET: aaaaaaaaaaaaaaaaaaaaaa it had to hit while i was on spring break ...\n",
      "RETRIEVED: GENERAL INFO: COVID-19 is caused by the SARS-CoV-2 virus and spreads primarily through respiratory d ...\n",
      "LABEL: Misinformation\n",
      "\n",
      "TWEET: ukrainian media registered the first confirmed case of the new coronavirus. ...\n",
      "RETRIEVED: GENERAL INFO: COVID-19 is caused by the SARS-CoV-2 virus and spreads primarily through respiratory d ...\n",
      "LABEL: Misinformation\n",
      "\n",
      "Data split sizes:\n",
      "Train: 720, Validation: 80, Test: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 720/720 [00:00<00:00, 3817.59 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 3025.81 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4180.74 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the RAG-based model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results: {'eval_loss': 0.004900779575109482, 'eval_accuracy': 1.0, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 1.1911, 'eval_samples_per_second': 167.907, 'eval_steps_per_second': 10.914, 'epoch': 3.0}\n",
      "\n",
      "Preparing baseline model (without retrieved knowledge)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 720/720 [00:00<00:00, 2755.31 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 3644.73 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4313.73 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the baseline model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>0.007020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating baseline model on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline results: {'eval_loss': 0.00698930723592639, 'eval_accuracy': 1.0, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 1.1277, 'eval_samples_per_second': 177.345, 'eval_steps_per_second': 11.527, 'epoch': 3.0}\n",
      "\n",
      "=== COMPARISON OF RESULTS ===\n",
      "Metric    | Baseline | RAG-based\n",
      "--------------------------------\n",
      "accuracy  | 1.0000 | 1.0000 (0.0000 (↓))\n",
      "f1        | 0.0000 | 0.0000 (0.0000 (↓))\n",
      "precision | 0.0000 | 0.0000 (0.0000 (↓))\n",
      "recall    | 0.0000 | 0.0000 (0.0000 (↓))\n",
      "\n",
      "=== CASE STUDIES ===\n",
      "Found 0 examples where models disagree\n",
      "\n",
      "Saving models...\n",
      "\n",
      "=== COMPLETED RAG-BASED HEALTH MISINFORMATION DETECTION ===\n"
     ]
    }
   ],
   "source": [
    "# RAG-Based Health Misinformation Detection for COVID-19 Tweets\n",
    "# Complete implementation with fixed training args and proper label handling\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import requests\n",
    "import time\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"Using transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1. Load and Preprocess the Dataset\n",
    "# ------------------------------------------\n",
    "df = pd.read_csv('merged_dataset.csv')  # Adjust path if needed\n",
    "print(f\"Loaded dataset with {len(df)} entries\")\n",
    "print(df.head())\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Convert labels to binary format\n",
    "df['label_encoded'] = df['label'].apply(lambda x: 1 if x == 'misinformation' else 0)\n",
    "\n",
    "# Basic preprocessing\n",
    "def preprocess_tweet(text):\n",
    "    \"\"\"Basic preprocessing for tweets\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove URLs (simple regex)\n",
    "        text = ' '.join([word for word in text.split() if not word.startswith('http')])\n",
    "        # Remove multiple spaces\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "df['processed_text'] = df['content'].apply(preprocess_tweet)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. External Knowledge Retrieval Functions\n",
    "# ------------------------------------------\n",
    "# For fact-checking sources (using a local database of COVID facts)\n",
    "def retrieve_from_factcheck(query):\n",
    "    \"\"\"Retrieve from local database of COVID facts\"\"\"\n",
    "    # Common COVID facts and misconceptions\n",
    "    covid_facts = {\n",
    "        \"covid cure\": \"There is no known cure for COVID-19, but vaccines are effective in preventing severe illness.\",\n",
    "        \"covid vaccine\": \"COVID-19 vaccines have been scientifically proven to be safe and effective.\",\n",
    "        \"5g covid\": \"There is no scientific evidence linking 5G technology to COVID-19.\",\n",
    "        \"mask\": \"Masks help reduce the spread of COVID-19 by blocking respiratory droplets.\",\n",
    "        \"hydroxychloroquine\": \"Studies have not shown hydroxychloroquine to be effective against COVID-19.\",\n",
    "        \"vitamin\": \"While vitamins support immune health, no vitamin has been proven to prevent or cure COVID-19.\",\n",
    "        \"microchip\": \"COVID-19 vaccines do not contain microchips or tracking devices.\",\n",
    "        \"bill gates\": \"Claims that Bill Gates is using vaccines for population control are false.\",\n",
    "        \"covid lab\": \"The scientific consensus is that COVID-19 was not artificially created in a laboratory.\",\n",
    "        \"covid hoax\": \"COVID-19 is a real disease that has caused millions of deaths worldwide.\",\n",
    "        \"covid fake\": \"COVID-19 is a real disease, not a hoax or conspiracy.\",\n",
    "        \"lockdown\": \"Lockdowns were implemented to slow the spread of COVID-19 and prevent healthcare systems from being overwhelmed.\",\n",
    "        \"pcr test\": \"PCR tests are reliable for detecting the presence of the SARS-CoV-2 virus that causes COVID-19.\",\n",
    "        \"covid deaths\": \"COVID-19 has caused millions of deaths globally, as confirmed by excess mortality studies.\",\n",
    "        \"covid origin\": \"Scientific evidence suggests COVID-19 originated from animal-to-human transmission.\",\n",
    "        \"covid symptoms\": \"Common COVID-19 symptoms include fever, cough, fatigue, and loss of taste or smell.\",\n",
    "        \"quarantine\": \"Quarantine helps prevent the spread of COVID-19 by isolating potentially infected individuals.\",\n",
    "        \"covid test\": \"COVID-19 tests are designed to detect current or past infection with the SARS-CoV-2 virus.\",\n",
    "        \"covid treatment\": \"COVID-19 treatments may include antivirals, monoclonal antibodies, or supportive care.\",\n",
    "        \"covid statistics\": \"COVID-19 case and death statistics are tracked by health organizations worldwide.\",\n",
    "        \"covid immunity\": \"Both vaccination and prior infection can provide some immunity against COVID-19.\",\n",
    "        \"covid variants\": \"COVID-19 variants emerge through genetic mutations in the SARS-CoV-2 virus.\",\n",
    "        \"vaccine side effects\": \"COVID-19 vaccines can cause temporary side effects like fatigue or soreness, but serious side effects are extremely rare.\",\n",
    "        \"ivermectin\": \"Medical authorities do not recommend ivermectin for COVID-19 treatment outside of clinical trials.\",\n",
    "        \"covid children\": \"Children can contract and transmit COVID-19, though they typically have milder symptoms than adults.\",\n",
    "        \"natural immunity\": \"Natural immunity from infection provides some protection, but vaccination is still recommended.\",\n",
    "        \"vaccine mandate\": \"Vaccine mandates have been implemented in some places to increase vaccination rates and protect public health.\",\n",
    "        \"covid restrictions\": \"COVID-19 restrictions were implemented to reduce transmission and save lives.\",\n",
    "        \"covid conspiracy\": \"Scientific evidence contradicts conspiracy theories about COVID-19's origin or purpose.\",\n",
    "        \"covid survival rate\": \"While many people survive COVID-19, it has caused millions of deaths worldwide.\",\n",
    "        \"wuhan\": \"The first identified cases of COVID-19 were in Wuhan, China in late 2019.\",\n",
    "        \"who covid\": \"The World Health Organization provides guidance on COVID-19 prevention, detection, and treatment.\",\n",
    "        \"cdc covid\": \"The CDC provides evidence-based guidance on COVID-19 for the United States.\",\n",
    "        \"covid pneumonia\": \"COVID-19 can cause pneumonia, a serious lung infection.\",\n",
    "        \"covid testing\": \"COVID-19 testing is an important tool for detecting and controlling the spread of the virus.\",\n",
    "        \"asymptomatic\": \"People with asymptomatic COVID-19 can still spread the virus to others.\",\n",
    "        \"covid vaccine safety\": \"COVID-19 vaccines have undergone rigorous safety testing and continuous monitoring.\",\n",
    "        \"covid hospitalization\": \"COVID-19 can lead to hospitalization, especially for unvaccinated individuals and those with risk factors.\",\n",
    "        \"long covid\": \"Some COVID-19 patients experience persistent symptoms, known as Long COVID.\",\n",
    "        \"false positive\": \"False positives in COVID-19 testing are possible but rare with PCR tests when performed correctly.\"\n",
    "    }\n",
    "    \n",
    "    # Check if any key phrases are in the query\n",
    "    for key, fact in covid_facts.items():\n",
    "        if key in query.lower():\n",
    "            return fact\n",
    "    \n",
    "    return \"No specific fact-check information found for this query.\"\n",
    "\n",
    "# Combined retrieval function that doesn't rely on external APIs\n",
    "def retrieve_knowledge(tweet):\n",
    "    \"\"\"Retrieve external knowledge for a tweet using only local data\"\"\"\n",
    "    # Extract key phrases from tweet (simplified approach)\n",
    "    words = tweet.lower().split()\n",
    "    query = \" \".join([w for w in words if len(w) > 3 and w not in ['this', 'that', 'with', 'from', 'what', 'when']])\n",
    "    \n",
    "    # Add 'covid' to the query if not present and the tweet is about COVID\n",
    "    if 'covid' not in query and ('covid' in tweet.lower() or 'coronavirus' in tweet.lower()):\n",
    "        query = 'covid ' + query\n",
    "    \n",
    "    # Limit query length\n",
    "    query = ' '.join(query.split()[:7])\n",
    "    \n",
    "    # Get information from fact-check source\n",
    "    factcheck_info = retrieve_from_factcheck(query)\n",
    "    \n",
    "    if factcheck_info != \"No specific fact-check information found for this query.\":\n",
    "        return \"FACT CHECK: \" + factcheck_info\n",
    "    \n",
    "    # If no specific fact check found, provide general COVID information\n",
    "    covid_general_info = {\n",
    "        \"general\": \"COVID-19 is caused by the SARS-CoV-2 virus and spreads primarily through respiratory droplets.\",\n",
    "        \"symptoms\": \"Common COVID-19 symptoms include fever, cough, fatigue, and loss of taste or smell.\",\n",
    "        \"prevention\": \"Preventive measures for COVID-19 include vaccination, masks, physical distancing, and hand hygiene.\",\n",
    "        \"treatment\": \"COVID-19 treatment may include antivirals, monoclonal antibodies, or supportive care depending on severity.\",\n",
    "        \"vaccine\": \"COVID-19 vaccines are safe, effective, and reduce risk of severe illness and hospitalization.\"\n",
    "    }\n",
    "    \n",
    "    # Select relevant general information\n",
    "    for key, info in covid_general_info.items():\n",
    "        if key in query:\n",
    "            return \"GENERAL INFO: \" + info\n",
    "    \n",
    "    return \"GENERAL INFO: \" + covid_general_info[\"general\"]\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3. Testing the Retrieval Function\n",
    "# ------------------------------------------\n",
    "# Let's test our retrieval function on a few examples\n",
    "test_tweets = df['processed_text'].iloc[:5].tolist()\n",
    "print(\"\\nTesting retrieval function on sample tweets:\")\n",
    "for tweet in test_tweets:\n",
    "    print(\"\\nTWEET:\", tweet[:100], \"...\")\n",
    "    knowledge = retrieve_knowledge(tweet)\n",
    "    print(\"RETRIEVED:\", knowledge[:100], \"...\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. Retrieve Knowledge for All Tweets\n",
    "# ------------------------------------------\n",
    "# Determine sample size: use full dataset if small, otherwise sample\n",
    "sample_size = min(1000, len(df))\n",
    "if len(df) > sample_size:\n",
    "    print(f\"\\nUsing a sample of {sample_size} tweets to avoid computational overhead\")\n",
    "    sampled_indices = np.random.choice(len(df), size=sample_size, replace=False)\n",
    "    df_sample = df.iloc[sampled_indices].copy()\n",
    "else:\n",
    "    df_sample = df.copy()\n",
    "\n",
    "# Retrieve knowledge for the sampled tweets\n",
    "print(\"\\nRetrieving external knowledge for tweets...\")\n",
    "df_sample['retrieved_knowledge'] = \"\"\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "    knowledge = retrieve_knowledge(row['processed_text'])\n",
    "    df_sample.at[idx, 'retrieved_knowledge'] = knowledge\n",
    "\n",
    "# Display a few examples\n",
    "print(\"\\nExamples of retrieved knowledge:\")\n",
    "for i in range(min(5, len(df_sample))):\n",
    "    print(\"\\nTWEET:\", df_sample['processed_text'].iloc[i][:100], \"...\")\n",
    "    print(\"RETRIEVED:\", df_sample['retrieved_knowledge'].iloc[i][:100], \"...\")\n",
    "    print(\"LABEL:\", df_sample['label'].iloc[i])\n",
    "\n",
    "# ------------------------------------------\n",
    "# 5. Prepare Data for RAG Model - 80/20 split\n",
    "# ------------------------------------------\n",
    "# Split into train (80%) and test (20%) sets\n",
    "train_df, test_df = train_test_split(df_sample, test_size=0.2, random_state=42, stratify=df_sample['label_encoded'])\n",
    "# Further split train into train and validation\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['label_encoded'])\n",
    "\n",
    "print(\"\\nData split sizes:\")\n",
    "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Create a new column combining tweet and retrieved knowledge\n",
    "def combine_text_and_knowledge(text, knowledge):\n",
    "    return f\"Tweet: {text} [SEP] Knowledge: {knowledge}\"\n",
    "\n",
    "train_df['combined_text'] = train_df.apply(lambda x: combine_text_and_knowledge(\n",
    "    x['processed_text'], x['retrieved_knowledge']), axis=1)\n",
    "val_df['combined_text'] = val_df.apply(lambda x: combine_text_and_knowledge(\n",
    "    x['processed_text'], x['retrieved_knowledge']), axis=1)\n",
    "test_df['combined_text'] = test_df.apply(lambda x: combine_text_and_knowledge(\n",
    "    x['processed_text'], x['retrieved_knowledge']), axis=1)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6. Define Model and Tokenizer\n",
    "# ------------------------------------------\n",
    "# We'll use a pre-trained model well-suited for tweet classification\n",
    "model_name = \"distilbert-base-uncased\"  # Smaller model for faster training\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 512  # Long enough for tweet + retrieved knowledge\n",
    "\n",
    "# Modified tokenization function that correctly includes labels\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['combined_text'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Add labels to the tokenized output\n",
    "    tokenized[\"labels\"] = examples[\"label_encoded\"]\n",
    "    return tokenized\n",
    "\n",
    "# Convert to HuggingFace datasets first\n",
    "train_dataset = Dataset.from_pandas(train_df[['combined_text', 'label_encoded']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['combined_text', 'label_encoded']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['combined_text', 'label_encoded']])\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for pytorch\n",
    "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# ------------------------------------------\n",
    "# 7. Define Performance Metrics\n",
    "# ------------------------------------------\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# ------------------------------------------\n",
    "# 8. Train the RAG-based Model\n",
    "# ------------------------------------------\n",
    "# Load pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)  # Move model to GPU if available\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define training arguments with more frequent checkpointing\n",
    "batch_size = 16\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=checkpoint_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=3,  # Keep only the last 3 checkpoints to save disk space\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision training if GPU available\n",
    "    report_to=\"none\"  # Disable wandb reporting\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining the RAG-based model...\")\n",
    "trainer.train()\n",
    "\n",
    "# ------------------------------------------\n",
    "# 9. Evaluate on Test Set\n",
    "# ------------------------------------------\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(f\"Test results: {test_results}\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 10. Compare with Baseline (No Knowledge)\n",
    "# ------------------------------------------\n",
    "print(\"\\nPreparing baseline model (without retrieved knowledge)...\")\n",
    "# Create datasets without retrieved knowledge\n",
    "train_df['tweet_only'] = train_df['processed_text']\n",
    "val_df['tweet_only'] = val_df['processed_text']\n",
    "test_df['tweet_only'] = test_df['processed_text']\n",
    "\n",
    "baseline_train = Dataset.from_pandas(train_df[['tweet_only', 'label_encoded']])\n",
    "baseline_val = Dataset.from_pandas(val_df[['tweet_only', 'label_encoded']])\n",
    "baseline_test = Dataset.from_pandas(test_df[['tweet_only', 'label_encoded']])\n",
    "\n",
    "# Tokenization function for baseline\n",
    "def tokenize_baseline(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['tweet_only'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = examples[\"label_encoded\"]\n",
    "    return tokenized\n",
    "\n",
    "tokenized_baseline_train = baseline_train.map(tokenize_baseline, batched=True)\n",
    "tokenized_baseline_val = baseline_val.map(tokenize_baseline, batched=True)\n",
    "tokenized_baseline_test = baseline_test.map(tokenize_baseline, batched=True)\n",
    "\n",
    "# Set format for pytorch\n",
    "tokenized_baseline_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_baseline_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_baseline_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "baseline_model.to(device)  # Move to GPU if available\n",
    "\n",
    "baseline_checkpoint_dir = './baseline_checkpoints'\n",
    "os.makedirs(baseline_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "baseline_training_args = TrainingArguments(\n",
    "    output_dir=baseline_checkpoint_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./baseline_logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=3,  # Keep only the last 3 checkpoints\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision training if GPU available\n",
    "    report_to=\"none\"  # Disable wandb reporting\n",
    ")\n",
    "\n",
    "baseline_trainer = Trainer(\n",
    "    model=baseline_model,\n",
    "    args=baseline_training_args,\n",
    "    train_dataset=tokenized_baseline_train,\n",
    "    eval_dataset=tokenized_baseline_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"\\nTraining the baseline model...\")\n",
    "baseline_trainer.train()\n",
    "\n",
    "# Evaluate baseline model\n",
    "print(\"\\nEvaluating baseline model on test set...\")\n",
    "baseline_results = baseline_trainer.evaluate(tokenized_baseline_test)\n",
    "print(f\"Baseline results: {baseline_results}\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 11. Compare Results\n",
    "# ------------------------------------------\n",
    "print(\"\\n=== COMPARISON OF RESULTS ===\")\n",
    "print(\"Metric    | Baseline | RAG-based\")\n",
    "print(\"--------------------------------\")\n",
    "for metric in ['accuracy', 'f1', 'precision', 'recall']:\n",
    "    baseline_value = baseline_results.get(f'eval_{metric}', 0)\n",
    "    rag_value = test_results.get(f'eval_{metric}', 0)\n",
    "    diff = rag_value - baseline_value\n",
    "    diff_str = f\"{diff:.4f} ({'↑' if diff > 0 else '↓'})\"\n",
    "    print(f\"{metric.ljust(10)}| {baseline_value:.4f} | {rag_value:.4f} ({diff_str})\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 12. Case Study - Qualitative Analysis\n",
    "# ------------------------------------------\n",
    "# Let's examine some examples where RAG and baseline models disagree\n",
    "print(\"\\n=== CASE STUDIES ===\")\n",
    "# Make predictions using both models\n",
    "baseline_preds = baseline_trainer.predict(tokenized_baseline_test)\n",
    "rag_preds = trainer.predict(tokenized_test)\n",
    "baseline_labels = baseline_preds.predictions.argmax(-1)\n",
    "rag_labels = rag_preds.predictions.argmax(-1)\n",
    "true_labels = test_df['label_encoded'].values\n",
    "\n",
    "# Find examples where models disagree\n",
    "disagreement_indices = np.where(baseline_labels != rag_labels)[0]\n",
    "print(f\"Found {len(disagreement_indices)} examples where models disagree\")\n",
    "\n",
    "# Select a few interesting examples for case study\n",
    "case_study_indices = disagreement_indices[:min(5, len(disagreement_indices))]\n",
    "for idx in case_study_indices:\n",
    "    tweet = test_df['processed_text'].iloc[idx]\n",
    "    knowledge = test_df['retrieved_knowledge'].iloc[idx]\n",
    "    true_label = \"Misinformation\" if true_labels[idx] == 1 else \"Reliable\"\n",
    "    baseline_pred = \"Misinformation\" if baseline_labels[idx] == 1 else \"Reliable\"\n",
    "    rag_pred = \"Misinformation\" if rag_labels[idx] == 1 else \"Reliable\"\n",
    "    \n",
    "    print(\"\\n---\")\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Retrieved Knowledge: {knowledge}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Baseline Prediction: {baseline_pred}\")\n",
    "    print(f\"RAG Model Prediction: {rag_pred}\")\n",
    "    \n",
    "    # Highlight which model was correct\n",
    "    if rag_labels[idx] == true_labels[idx] and baseline_labels[idx] != true_labels[idx]:\n",
    "        print(\"✓ RAG model was correct, baseline was wrong\")\n",
    "    elif baseline_labels[idx] == true_labels[idx] and rag_labels[idx] != true_labels[idx]:\n",
    "        print(\"✗ Baseline was correct, RAG model was wrong\")\n",
    "    else:\n",
    "        print(\"Both models were incorrect\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 13. Save Models and Results\n",
    "# ------------------------------------------\n",
    "# Save the trained RAG model\n",
    "print(\"\\nSaving models...\")\n",
    "trainer.save_model('./rag_model')\n",
    "baseline_trainer.save_model('./baseline_model')\n",
    "\n",
    "# Save results summary\n",
    "results_summary = {\n",
    "    'RAG': {k: float(v) for k, v in test_results.items()},  # Convert numpy values to Python native types\n",
    "    'Baseline': {k: float(v) for k, v in baseline_results.items()},\n",
    "    'Sample_Size': len(df_sample)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('./results_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f)\n",
    "\n",
    "print(\"\\n=== COMPLETED RAG-BASED HEALTH MISINFORMATION DETECTION ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T05:33:01.102981Z",
     "iopub.status.busy": "2025-04-15T05:33:01.102161Z",
     "iopub.status.idle": "2025-04-15T05:33:15.994076Z",
     "shell.execute_reply": "2025-04-15T05:33:15.992847Z",
     "shell.execute_reply.started": "2025-04-15T05:33:01.102953Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.1\n",
      "    Uninstalling transformers-4.51.1:\n",
      "      Successfully uninstalled transformers-4.51.1\n",
      "Successfully installed transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7128467,
     "sourceId": 11384364,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
